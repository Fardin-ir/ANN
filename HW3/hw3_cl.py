# -*- coding: utf-8 -*-
"""HW3-cl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cN2x2ZxsmaJowoDzWL_l61ewYoyFZGWE
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from matplotlib import pyplot as plt
import math
from sklearn.preprocessing import StandardScaler
import datetime
import os
from sklearn.preprocessing import LabelEncoder
from skimage import io
from sklearn.model_selection import train_test_split

def create_data_set(x_crop=150, y_crop=150, train_size=0.9):
    images_path = [os.path.join("yalefaces", item) for item in os.listdir("yalefaces")]
    image_data = []
    image_labels = []

    for i, im_path in enumerate(images_path):
        im = io.imread(im_path, as_gray=True)
        image_data.append(np.array(im, dtype='uint8').ravel())

        label = os.path.split(im_path)[1].split(".")[1]
        image_labels.append(label)

    X_ = np.array(image_data).astype('float32')
    enc = LabelEncoder()
    y_ = enc.fit_transform(np.array(image_labels))
    print(enc.classes_)
    X_train, X_test, y_train, y_test = train_test_split(X_, y_, train_size=train_size, random_state=22)

    return X_train, X_test, y_train, y_test

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import accuracy_score
# %load_ext tensorboard

from sklearn.metrics import confusion_matrix
import seaborn as sn

def train_test_valid_data(data,frac_train,frac_valid):
    n_train = math.floor(frac_train*data.shape[0])
    n_valid = math.floor(frac_valid*data.shape[0])
    train_data = data[:n_train]
    valid_data = data[n_train:n_train+n_valid]
    test_data = data[n_train+n_valid:]
    return train_data,valid_data,test_data

class EarlyStoppingAtMinLoss(keras.callbacks.Callback):
    def __init__(self, patience=5):
        super(EarlyStoppingAtMinLoss, self).__init__()
        self.patience = patience
        # best_weights to store the weights at which the minimum loss occurs.
        self.best_weights = None

    def on_train_begin(self, logs=None):
        # The number of epoch it has waited when loss is no longer minimum.
        self.wait = 0
        # The epoch the training stops at.
        self.stopped_epoch = 0
        # Initialize the best as infinity.
        self.best = np.Inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get("val_loss")
        if np.less(current, self.best):
            self.best = current
            self.wait = 0
            # Record the best weights if current results is better (less).
            self.best_weights = self.model.get_weights()
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                print("Restoring model weights from the end of the best epoch.")
                self.model.set_weights(self.best_weights)

    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0:
            print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))

"""# **Part A - Original Data**"""

'''
X_train, X_test, y_train, y_test = create_data_set()
y_train = tf.keras.utils.to_categorical(y_train, num_classes=11)
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print(X_train.shape)
print(X_test.shape)
'''



"""## **Part B - Reduced Data** - Comment out this part for Part A"""

X_train = pd.read_csv('re/X_train.csv', header=None).values
X_test = pd.read_csv('re/X_test.csv', header=None).values
y_train = pd.read_csv('re/y_train.csv', header=None).values.ravel()
print(y_train)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=11)
y_test = pd.read_csv('re/y_test.csv', header=None).values.ravel()

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


#Model
keras.backend.clear_session()
  
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

inp = keras.Input(shape=(X_train.shape[1]), name="input")
x = keras.layers.Dense(units=50,  name="h1", activation='relu')(inp)
x1 = keras.layers.Dense(name="output1", units=11, activation='softmax')(x)

model = keras.Model(inputs=inp, outputs=x1)

model.compile(loss = keras.losses.CategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])
model.fit(X_train[:], y_train[:], epochs=10000, verbose=1, validation_split=0.25,callbacks=[EarlyStoppingAtMinLoss(5),tensorboard_callback])

pred_test = model.predict(X_test)
pred_test = pred_test.argmax(axis=-1)
print(y_test)
print("Accuracy for test:", accuracy_score(y_test,pred_test))

cm = confusion_matrix(y_test, pred_test, np.unique(y_test))
df_cm = pd.DataFrame(cm, index = [i for i in np.unique(y_test)],
                  columns = [i for i in np.unique(y_test)])
print(df_cm)

