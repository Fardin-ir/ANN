# -*- coding: utf-8 -*-
"""HW2-cl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cN2x2ZxsmaJowoDzWL_l61ewYoyFZGWE
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from matplotlib import pyplot as plt
import math
from sklearn.preprocessing import StandardScaler
import datetime

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import accuracy_score
# %load_ext tensorboard

from sklearn.metrics import confusion_matrix
import seaborn as sn

def train_test_valid_data(data,frac_train,frac_valid):
    n_train = math.floor(frac_train*data.shape[0])
    n_valid = math.floor(frac_valid*data.shape[0])
    train_data = data[:n_train]
    valid_data = data[n_train:n_train+n_valid]
    test_data = data[n_train+n_valid:]
    return train_data,valid_data,test_data

class EarlyStoppingAtMinLoss(keras.callbacks.Callback):
    def __init__(self, patience=5):
        super(EarlyStoppingAtMinLoss, self).__init__()
        self.patience = patience
        # best_weights to store the weights at which the minimum loss occurs.
        self.best_weights = None

    def on_train_begin(self, logs=None):
        # The number of epoch it has waited when loss is no longer minimum.
        self.wait = 0
        # The epoch the training stops at.
        self.stopped_epoch = 0
        # Initialize the best as infinity.
        self.best = np.Inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get("val_loss")
        if np.less(current, self.best):
            self.best = current
            self.wait = 0
            # Record the best weights if current results is better (less).
            self.best_weights = self.model.get_weights()
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                print("Restoring model weights from the end of the best epoch.")
                self.model.set_weights(self.best_weights)

    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0:
            print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))

def split_digits(n):
    numbers = y
    digits = np.unravel_index(numbers, 6*(10,))
    return np.stack([numbers, *digits], axis=1)

data = pd.read_csv("drive/MyDrive/s_data.csv", header=None)

y = data[0]
X = data.drop(0, axis=1)
X_train, X_valid, X_test = train_test_valid_data(X, 0.7, 0.2)
y_train, y_valid, y_test = train_test_valid_data(y, 0.7, 0.2)
min = y_train.min()
y_train = y_train - min
y_train = tf.keras.utils.to_categorical(y_train, num_classes=90)
y_valid = y_valid - min
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=90)
y_test = y_test - min

plt.hist(y)
plt.show()

print(y_train)

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_valid = scaler.transform(X_valid)

print(X_train.shape)
print(y_train.shape)

keras.backend.clear_session()
def adapt_learning_rate(epoch):
    return 0.001 / ((epoch+1)/10)
  
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
my_lr_scheduler = keras.callbacks.LearningRateScheduler(adapt_learning_rate)

inp = keras.Input(shape=(90), name="input")
x = keras.layers.Dense(units=100,  name="h1", activation='relu')(inp)
x1 = keras.layers.Dense(name="output1", units=90, activation='softmax')(x)

model = keras.Model(inputs=inp, outputs=x1)

model.compile(loss = keras.losses.CategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(0.0008), metrics=['accuracy'])
model.fit(X_train[:], y_train[:], epochs=10000, verbose=1, validation_data=(X_valid,y_valid),callbacks=[EarlyStoppingAtMinLoss(5),tensorboard_callback])

pred_test = model.predict(X_test)
pred_test = pred_test.argmax(axis=-1)
print("Accuracy for test:", accuracy_score(y_test,pred_test))

cm = confusion_matrix(y_test, pred_test, np.unique(y_test))
df_cm = pd.DataFrame(cm, index = [i for i in np.unique(y_test)+min],
                  columns = [i for i in np.unique(y_test)+min])
df_cm = df_cm.iloc[:-20,:-20]
plt.figure(figsize = (20,14))
sn.heatmap(df_cm)

tensorboard --logdir logs/fit/20210423-131405